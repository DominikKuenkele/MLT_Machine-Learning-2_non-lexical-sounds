%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fphw Assignment
% LaTeX Template
% Version 1.0 (27/04/2019)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Class by Felipe Portales-Oliva (f.portales.oliva@gmail.com) with template 
% content and modifications by Vel (vel@LaTeXTemplates.com)
%
% Template (this file) License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	11pt, % Default font size, values between 10pt-12pt are allowed
]{fphw}

% Template-specific packages
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Use the Palatino font

\usepackage{graphicx} % Required for including images

\usepackage{booktabs} % Required for better horizontal rules in tables

\usepackage{listings} % Required for insertion of code

\usepackage{enumerate} % To modify the enumerate environment

\usepackage{color}
\usepackage{todonotes}
\usepackage{scrextend}
\usepackage{caption}
\usepackage{nameref}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{framed}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{float}
\usepackage{amsmath}

\restylefloat{table}

\definecolor{color_background}{rgb}{0.98,0.98,0.98}
\definecolor{shadecolor}{rgb}{0.98,0.98,0.98}

\newenvironment{code}
    {\captionsetup{
        type=listing,
        skip=2pt,
        belowskip=15pt
        }}
    {}

\setminted{
    linenos=true, 
    frame=lines,
    breaklines=true,
    bgcolor=color_background,
    }
\usemintedstyle{one-dark}
\setmintedinline{breaklines}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Project: Non-lexical sounds in dialogue utterances} % Assignment title

\date{November 30th, 2022} % Due date

\author{Dominik KÃ¼nkele}

\institute{University of Gothenburg} % Institute or school name

\class{Machine Learning 2 (LT2326)} % Course or class name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the assignment title, created automatically using the information in the custom commands above

%----------------------------------------------------------------------------------------
%	ASSIGNMENT CONTENT
%----------------------------------------------------------------------------------------

\section*{Background}
In one of their papers, Edlund and his colleagues describe human interaction with machines using metaphors on a spectrum \cite{edlund}. On the one side of the spectrum is the so-called \emph{interface metaphor}. The interface of the machine is built in a way that users are aware, they are talking to a machine. Consequently, they also adapt their language, and use more command like utterances (e.g. "Call John", "Set the timer") as they would fill slots in an imaginary web form. On the other end of the spectrum resides the \emph{human metaphor}. Here, the users don't really know, they are talking to a machine and therefore use a "normal" language in the sense of utterances, they would use in a human-human dialogue. While the interface metaphor is still used very commonly, many dialogue systems like \emph{Alexa} or \emph{Siri} lie somewhere in between these extremes. The human metaphor is implemented rarely and is seen more often in science fiction.

Nevertheless, there are a few strong reasons for using the human metaphor and let the users talk in natural language. Natural language is
\begin{description}
    \item[easy to use.] Since we use natural language all the time in all human interactions, it is very natural for us, to also use it in machine interaction.
    \item[flexible.] Natural language allows us to express everything we want to express. We can express for instance thoughts, feelings or facts with different certainties or also for example talk about things that were, things that are, and some things that have not yet come to pass. There are only very few things in a human mind that cannot be represented in natural language (partly in combination with mimic and gestures).
    \item[resilient to error handling.] Furthermore, it allows us, to correct things we said or also specify certain parts more if we realize the listener is not understanding very easily. In the role of the listener, we can also verify, what we understood. Both can be done in various ways, by for instance rephrasing the utterance, repeating all or parts of it or even ask questions about it.
    \item[enjoyable] Using Natural Language is finally also much more enjoyable than using a command like language. This of course depends on the person, but in general, humans evolved to use and interact in this language.
\end{description}

The goal is now, not to create a machine that is very close to humans, but to create "[a] machine that acts human enough that we respond to it as we respond to another human" as Cassel puts it in his paper in 2007 \cite{cassel}.

There are endless factors that could define, \emph{what} human-like interaction is. They include for example a change in pitch (upwards, to signal turn-keeping; downwards, to signal turn-yielding) or choices of words ("This is the automated booking system" will obviously signal the user that it speaks to a machine, while "Hello" does not). One big factor are non-lexical sounds (NLS) like hesitations ("uh", "hmm", ...) or repetitions of words/n-grams ("if I'm home then I I definitely watch her"). This could be famously observed in the \emph{Google Duplex} demo in 2018, when a machine was calling a hair saloon using NLS.

This project aims to make machine utterances more human-like, by adding NLS to an utterance at natural positions. Additionally, the utterances could also be enhanced, by adding repetitions of n-grams.

\section*{Data resource}
As a resource for this project, I use the \emph{Switchboard corpus} \cite{switchboard}. There are various dialogue corpora published, but the Switchboard corpus contains NLS in a mostly structured way. The rest of the dialogue is kept mostly undistilled with annotated changes and normalizations. It tries to be as close as possible to the original recordings. This means that the corpus includes
\begin{description}
    \item[non-grammatical sentences:] "yeah i think it is too it's gonna get better"
    \item[repetitions of n-grams:] "oh she didn't she didn't do something"
    \item[annotated partial words:] "bec- because we're so"
    \item[anomalies of words with correct word] "bettle/better"
\end{description}

Furthermore, the corpus classifies the NLS based on there sound into thirteen classes: ah, eh, hm, huh, huh-uh, hum-um, ooh, uh, uh-huh, uh-hum, uh-oh, um, um-hum. These classes will be used for predicting the NLS in the generated sentences.

I cleansed the corpus, by removing annotations, \emph{silences} and laughters, replacing partial with complete words and NLS with a specific token per class. In the end, I could extract 247 123 utterances. The length of these utterances ranges from 1 to 81 words. Since I only used each utterance by itself to generate NLS and was not using any relations in the dialogue, I also excluded utterances shorter than two words. That yielded me around 150 000 usable utterances. Around 70 000 of these utterances contain at least one NLS.

Almost half of the utterances contain a repetition. Most of the repetitions are only one word, but I could identify also repetitions of up to 9-grams.

\section*{Methods}
The idea of this project was to predict if and which NLS should follow each word in a sentence. The model consists of (A) an embedding layer for tokens and POS, (B) encoder layers for tokens and POS and (C) a classifier that classifies the NLS for the output of the encoding at each encoding step.

\subsection*{(A) Embedding layer}
In the beginning, the embeddings for tokens and POS were trained from scratch. Here, I used my own built-up vocabulary from the training data, containing also the special tokens for START, END, UNKNOWN and PADDING. With this, I also could vary the embedding size. Later, I switched to pretrained \emph{GloVe} embeddings for the token to help the model to understand the relation between the words easier from the start. I utilized the GloVe embeddings that are based on 6 billion tokens embedded in 300 dimensions. I added the special tokens \emph{<SOS>}, \emph{<END>}, \emph{<PAD>} and \emph{<UNK>} with random initializations to its vocabulary. Since the embeddings are already pretrained, while the rest of my model is randomly initialized, I froze them for the first 10 epochs. After that, they are fine-tuned together with the rest of the model.

\subsection*{(B) Encoding layer}
The next step was to encode the sentence step by step (word by word). The simplest solution would have been, to just use a LSTM layer and encode the embeddings automatically. In the project however, I wanted to see, how the model performs if I feed it different kind of information and encode them differently. In especially, I wanted to see if provide the model explicitly syntactical information by giving it the parts of speech (POS).

Therefore, the model is able to receive word embeddings and POS embeddings. Both embeddings are then encoded (1) a bidirectional LSTM encoder and (2) a bidirectional LSTM encoder for the POS. Furthermore, the word embeddings are additionally encoded with (3) a transformer encoder. These layers are then combined for each step in an LSTM by either concatenation or addition, optionally weighted as following:
\begin{enumerate}
    \item $concat(token\_LSTM, POS\_LSTM, transformer)$
    \item $concat(token\_LSTM, POS\_LSTM)$
    \item $token\_LSTM + POS\_LSTM$
    \item $token\_LSTM + 0.5 * POS\_LSTM$
    \item $token\_LSTM$
\end{enumerate}

\subsection*{(C) Clasifier}
The classifier is built on top of each step of the LSTMs. This allows me to predict an NLS token in every position of the sentence. For this, the classifier finally combines multiple linear layers together with non-linear functions. This reduced the dimension from the output dimension of the combination of the encoders to 14 classes (NLS tokens + <NO\_NLS> token). The number of encoder dimensions differs depending on the way, the encoders are combined. If the number was very high (e.g. for the concatenation of both LSTMs and the transformer), I introduced an additional linear layer.

For all the different layers, I applied dropout with a probability of 0.2 during training. This should prevent the model from overfitting to the training data.

\subsection*{Input for the model}
As explained before, I needed to feed the model the words aligned with POS. The example sentence \emph{uh-huh i think so and uh} should be represented like this with an output by the model:

\begin{table}[h]
    \centering
    \begin{tabular}{c || c c c c c c c}
        input  tokens & <SOS>  & i        & think    & so       & and & <EOS>    \\
        input POS     & <SOS>  & NN       & VBP      & RB       & CC  & <EOS>    \\
        \hline
        output        & uh-huh & <NO-NLS> & <NO-NLS> & <NO-NLS> & uh  & <NO-NLS>
    \end{tabular}
\end{table}

For this, I first tokenized the sentences, using \emph{nltk}s \emph{word\_tokenize} function. Afterwards, I excluded the NLS, added start-of-sequence and end-of-sequence tokens. This is then aligned with the extracted NLS enriched with <NO\_NLS> tokens if there is no NLS at a specific position.

\section*{Results and Discussion}
Evaluating the models was quite difficult. Since it is not a standard classification task, but generation of sequences, classic metrics like \emph{accuracy}, \emph{precision} or \emph{recall} may not yield meaningful value. The whole problem could be more seen as a translation task from 'English without NLS' to 'English with NLS'. That's why, I also included the \emph{Meteor score}, which is usually used in Machine Translation. Still, compared to Machine Translation, the difference between the source and the target sentence is very small, in most of the cases just one token. The Meteor score will therefore always be very high and differences between models may not be significant looking at this metric.

Furthermore, the placement of NLS (as well as the type of NLS) in a sentence is very subjective. NLS don't carry usually as much semantic meaning as normal words. Additionally, there are no grammar rules, when NLS may be used or not. This leads to the fact that every human may use these NLS differently and there is now objective 'truth' to there placement (which is rather the case with a normal grammar). Misplaced NLS by the model based on the training and test data may therefore be actually not misplaced based on human judgement. Consider the following two sentences:
\begin{center}
    (1) that's contrary to \textbf{uh} popular belief you know
\end{center}
\begin{center}
    (2) that's contrary \textbf{uh} to popular belief you know
\end{center}
Sentence 1 is picked from the training data. If the model predicted Sentence 2, all the metrics would punish the model even though for humans, both sentences sound acceptable. In the end, all the metrics can only give pointers and hints for this problem, which model might perform better than others. A human evaluation would be definitely necessary to judge the models' performance better.

\begin{table}[h]
    \centering
    \begin{tabular}{c || c c | c | c c c c c c c}
            & \multicolumn{2}{c|}{contain NLS} &               &                         &                &                &                &                &                & \\
            & gold                             & predicted     & $\frac{NLS}{<NO\_NLS>}$ & Accuracy       & Precision      & Recall         & Meteor         & NLS score        \\
        \hline
        (1) & 100                              & 41.31         & 6.51                    & 90.32          & 47.57          & 50.03          & 93.55          & 48.37            \\
        (1) & 36.80                            & 4.63          & 2.70                    & \textbf{96.79} & \textbf{79.19} & \textbf{80.64} & 97.16          & \textbf{49.70}   \\
        (2) & 36.80                            & 4.45          & 2.70                    & 96.70          & 78.59          & 79.97          & 97.18          & 49.66            \\
        (3) & 36.80                            & \textbf{4.98} & 2.70                    & 96.67          & 78.52          & 79.88          & \textbf{97.20} & 49.65            \\
        (4) & 36.80                            & 4.63          & 2.70                    & 96.69          & 78.56          & 79.93          & 97.19          & 49.66            \\
        (5) & 36.80                            & 4.86          & 2.70                    & 96.66          & 78.46          & 79.82          & 97.19          & 49.64
    \end{tabular}
    \caption{Evaluation of each model}
\end{table}

I tested the models with two different datasets. In one dataset, I kept all the utterances, independently if they contained NLS. For this, the test split consists of around 36.8\% utterances that contain at least one NLS. In the second dataset, I filtered out all sentences that don't contain any NLS. The proportion is therefore 100\%.
Since the test data differs from dataset to dataset, the metrics achieved with them can't be compared directly.

The reason for this was that the models generated almost no NLS at all using the unfiltered dataset. The results consisted only of 4.4\% to 5\% of sentences that contain NLS (compared to 36.8\% of the gold data). Using the filtered data set, 41\% of the generations contained NLS (compared to 100\% of the gold data).

For the unfiltered dataset, all the models produce very high metrics. The accuracy, as well as the Meteor score don't differ significantly across the models. Also the margin for the precision and recall lie under 0.6\% and 0.8\% respectively, which doesn't seem very significant.

My model has very high scores for all the metrics \emph{Accuracy}, \emph{Precision} and \emph{Recall}, even though the predicted sentences are not really good. The reason for that is only a very small proportion of the slots are NLS, while the biggest proportion is the <NO\_NLS> token. Therefore, my model that often only predicts <NO\_NLS> tokens for all the slots performs very well. To make this problem better visible, I introduced a new metric, called the \emph{NLS score}. It is a weighted accuracy that weights <NO\_NLS> tokens anti-proportionally to its occurrence in the test corpus. More specifically the weights are calculated as following for the whole test corpus:

\begin{align}
    W_{NLS}     & = \frac{number\ of\ <NO\_NLS>\ tokens}{number\ of\ slots} \\
    W_{NO\_NLS} & = 1 - W_{NLS}
\end{align}

The weights are then applied to each token $t$ of a sentence, depending on the gold label. If the gold label was a <NO\_NLS> token, $W_{NO\_NLS}$ is applied, otherwise $W_{NLS}$. The sum of the weighted scores is then normalized over the length of the sentence $n$ and averaged over all sentences.
\begin{align}
    A_{W,sentence} & = \frac{W_{NLS} * \sum_{}^{t}{correct\ NLS} + W_{NO\_NLS} *\sum_{}^{t}{correct\ NO\_NLS}}{t} \\
    A_W            & = \frac{\sum_{}^{n}A_{WS}}{n}
\end{align}

Finally, the NLS score is the mean of the resulting weighted accuracy with the average 'normal' accuracy.

\begin{equation}
    NLS\ score = \frac{A_W + A}{2}
\end{equation}

This new metric takes the imbalance of <NO\_NLS> and NLS tokens better into account. It is also flexible in respect to a changing ratio, since it could also handle the opposite case if the biggest portion of slots were NLS tokens. Furthermore, it lets me compare the filtered and unfiltered dataset a little better.

As expected the NLS score is much lower for all different models. But also here, the differences between the models are not significant. All models consistently produce too few sentences with NLS. Even using the filtered dataset yields a similar low NLS score.

Next, I looked at the predicted sentences that contained NLS. Most of the NLS, that were predicted were 'uh' or 'um'. It was hard to see any pattern, in which position they appeared. They appeared mostly in the middle of the sentence or towards the end. However, many of them were predicted before a noun or after conjunctions like 'that' or 'but'. Still, there appeared also a lot of NLS, which I couldn't classify together and understand, why they may have been predicted.

\section*{Conclusion}
Currently, the model doesn't produce sentences with NLS consistently. There are multiple ways, how the model could be improved in the future. One option could be to change the calculation of the loss. Since the cross entropy, I currently use is affected by the above described problem of imbalance of NLS and <NO\_NLS> tokens, a different calculation may be a better basis for the loss.

Furthermore, the dataset could be improved. First, I could use a bigger dataset, that represents a wider range of utterances and uses of NLS. Secondly, the data could be cleansed from unnecessary parts. Right now, the data holds mostly ungrammatical sentences with repetitions of n-grams, unsyntactical constructions and misspelled words. While this represents, how humans talk, it is maybe not the best basis to translate from grammatical correct 'utterances without NLS' to 'utterances with NLS'. Therefore, it could be attempted, to remove the ungrammaticalities and just leave the NLS. This could provide better results for multiple reasons: 
\begin{itemize}
    \item less unknown words and therefore better embeddings
    \item patterns in one utterance may repeat in more other utterances, since they are normalized with grammar rules. The model may detect these patterns easier
    \item POS tagger works better, since they are trained on grammatical sentences 
\end{itemize}

In a next step, the model can be adapted. Right now, I am using a straight forward model, concatenating different encodings and running them through linear layers. Another option could be for example to use a transformer and masking the NLS tokens. Going on step further, I could also try to take the surrounding context into account, i.e. the previous utterance of the dialogue partner. NLS in a sentence especially in the beginning may depend on what the dialogue partner said and what was understood. This could yield better results, since the model would have more information to analyze and detect patterns. 

In the end, it may also be helpful to make a deeper analysis of NLS and see, where they actually appear and why. The results of this analysis could help to build a more targeted model.

For the future, it may also be worth to have a look into silences and repetitions of phrases. These properties seem to be correlated with NLS and depend on each other. In the dataset, an NLS for example often occurs between repetitions. Using this knowledge may give better results for predicting human-like language.

\begin{thebibliography}{9}
    \bibitem{edlund}
    Edlund, J., Gustafson, J., Heldner, M., Hjalmarsson, A. (2007) \emph{Towards
human-like spoken dialogue systems}.
    
    \bibitem{cassel}
    Cassell, J. (2007) \emph{Body language: lessons from the near-human}.

    \bibitem{switchboard}
    ISIP (2003) \emph{Switchboard Corpus}, https://isip.piconepress.com/projects/switchboard/.
    \end{thebibliography}
\end{document}
