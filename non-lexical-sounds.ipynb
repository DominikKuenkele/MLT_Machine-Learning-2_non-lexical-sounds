{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "SWB_DIR = os.path.join(DATA_DIR, 'swb_ms98_transcriptions/')\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, 'train.tsv')\n",
    "TEST_PATH = os.path.join(DATA_DIR, 'test.tsv')\n",
    "\n",
    "\n",
    "SILENCE = '<silence>'\n",
    "NLS = [\n",
    "    'ah',\n",
    "    'eh', # pronouned 'eh'\n",
    "    'eh', # pronouned 'ey'\n",
    "    'hm',\n",
    "    'huh',\n",
    "    'huh-uh',\n",
    "    'hum-um',\n",
    "    'ooh',\n",
    "    'uh',\n",
    "    'uh-huh',\n",
    "    'uh-hum',\n",
    "    'uh-oh',\n",
    "    'um',\n",
    "    'um-hum',\n",
    "]\n",
    "\n",
    "PADDING_TOKEN = '<PAD>'\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "START_TOKEN = '<SOS>'\n",
    "END_TOKEN = '<EOS>'\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 16,\n",
    "    'embedding_dim': 256,\n",
    "    'lstm_out_dim': 512,\n",
    "    'epochs': 150,\n",
    "    'learning_rate': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteranceCollector():\n",
    "    ANNOTATIONS = [\n",
    "        #     r'\\[silence\\]', # may be also a sign of hesitation\n",
    "        r'\\[noise\\]',\n",
    "        r'\\[laughter\\]',\n",
    "        r'\\[vocalized-noise\\]'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, path, max_number_files=-1) -> None:\n",
    "        # nlp = English()\n",
    "        # nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "        self.utterances = []\n",
    "        for file_index, filename in enumerate(glob.iglob(os.path.join(path + '**/*trans*'), recursive=True)):\n",
    "            if file_index == max_number_files:\n",
    "                break\n",
    "\n",
    "            folders = filename.split('/')\n",
    "            dialogue_id = folders[-2]\n",
    "            dialogue_partner = folders[-1][folders[-1].find(dialogue_id) + len(dialogue_id)]\n",
    "\n",
    "            with open(filename, 'r') as f:\n",
    "                saved_utterances = []\n",
    "                for line in f:\n",
    "                    utterance = self.cleanse_utterance(line)\n",
    "                    self.utterances.append({\n",
    "                            'dialogue_id': dialogue_id,\n",
    "                            'dialogue_partner': dialogue_partner,\n",
    "                            'utterance': utterance\n",
    "                        })\n",
    "                    # if utterance == SILENCE and len(saved_utterances) != 0:\n",
    "                    #     # doc = nlp(' '.join(saved_utterances))\n",
    "                    #     # for sentence in doc.sents:\n",
    "                    #     #     utterances.append({\n",
    "                    #     #         'dialogue_id': dialogue_id,\n",
    "                    #     #         'dialoge_partner': dialogue_partner,\n",
    "                    #     #         'utterance': sentence.text\n",
    "                    #     #     })\n",
    "                    #     self.utterances.append({\n",
    "                    #             'dialogue_id': dialogue_id,\n",
    "                    #             'dialogue_partner': dialogue_partner,\n",
    "                    #             'utterance': ' '.join(saved_utterances)\n",
    "                    #         })\n",
    "                    #     saved_utterances = []\n",
    "\n",
    "                    # elif utterance != '' and utterance != SILENCE:\n",
    "                    #     saved_utterances.append(utterance)\n",
    "\n",
    "    def cleanse_utterance(self, utterance: str):\n",
    "        utterance = utterance.rstrip().split(' ', maxsplit=3)[-1]\n",
    "\n",
    "        # remove annotations\n",
    "        utterance = re.sub(fr'({\"|\".join(self.ANNOTATIONS)})', '', utterance)\n",
    "\n",
    "        # replace anomalous words.\n",
    "        # E.g.: \"... [bettle/better] ...\" -> \"... better ...\".\n",
    "        # Also prevent duplications: \"... [bettle/better] better ...\" -> \"... better ...\"\n",
    "        utterance = re.sub(r\"(^| )\\[(.*?)\\/(?P<replace>.*?)\\]( (?P=replace))?( |$|-)\", lambda x: f' {x.group(3)} ', utterance)\n",
    "\n",
    "        # replace words containing laughter.\n",
    "        # E.g.: \"... [laughter-alone] ...\" -> \"... alone ...\"\n",
    "        utterance = re.sub(r\"(^| )\\[laughter-(.*?)\\]( |$|-)\", lambda x: f' {x.group(2)} ', utterance)\n",
    "\n",
    "        # exclude too complicated annotations to replace automatically\n",
    "        if utterance.find(' [') > -1:\n",
    "            return ''\n",
    "        \n",
    "        # replace partial word pronounciations\n",
    "        # E.g. \"... pla[stic]- ...\" -> \"... plastic- ...\"\n",
    "        utterance = re.sub(r'\\[silence\\]', SILENCE, utterance)\n",
    "        utterance = re.sub(r'(\\[|\\])', '', utterance)\n",
    "\n",
    "        # remove duplicate blanks\n",
    "        utterance = re.sub(r' +', ' ', utterance).rstrip()\n",
    "\n",
    "        return utterance   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_nls(utterance: str):\n",
    "    return any(nls in utterance['utterance'] for nls in [*NLS, SILENCE])\n",
    "\n",
    "def contains_repetition(utterance: str, ngram=1):\n",
    "    split_utterance = utterance['utterance'].split(' ')\n",
    "    # include partial word pronounciations\n",
    "    split_utterance = [word.rstrip('-') for word in split_utterance]\n",
    "    zipped = list(zip(*[split_utterance[i:] for i in range(ngram)]))\n",
    "    return any(zipped[index] == zipped[index - ngram] for index in range(ngram, len(zipped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 391593\n",
      "contain nls: 262547\n",
      "contain 1-gram repetitions: 47199\n",
      "that's that's wonderful\n",
      "contain 2-gram repetitions: 9994\n",
      "how they react to that and everything but so i think i think you're right though they're getting little better and better sooner- sooner- you know sooner or later they're going to make some major breakthroughs\n",
      "contain 3-gram repetitions: 1823\n",
      "yeah well- we didn't have- we didn't have any that were that\n",
      "contain 4-gram repetitions: 428\n",
      "can't stand Jimmy Johnson or Jerry well uh you know i was just trying i was just trying to think of uh who who our coaches of Houston Oilers you probably know\n",
      "contain 5-gram repetitions: 100\n",
      "just to see the show just to see the show right\n",
      "contain 6-gram repetitions: 21\n",
      "oh yeah i lost twenty five pounds yeah i lost twenty five pounds now you can do the same oh yeah\n",
      "contain 7-gram repetitions: 9\n",
      "oh gosh yeah how do you usually cook your deer how do you usually cook your deer\n",
      "contain 8-gram repetitions: 5\n",
      "how long would it be in the microwave- how long would it be in the microwave\n",
      "contain 9-gram repetitions: 3\n",
      "each- each- each child is an individual and uh to be treated at a very very early age as a math and say you have to do this at a certain time you have to do this at a certain time it just wasn't working out real well\n",
      "Lengths:\n",
      "1: 207245 172589 172589 0\n",
      "2: 18845 11390 8778 2612\n",
      "3: 10477 4423 3578 845\n",
      "4: 8237 3485 2866 619\n",
      "5: 7611 3212 2590 622\n",
      "6: 7237 3251 2492 759\n",
      "7: 7015 3290 2452 838\n",
      "8: 6661 3389 2457 932\n",
      "9: 6328 3441 2476 965\n",
      "10: 6042 3567 2528 1039\n",
      "11: 5693 3655 2524 1131\n",
      "12: 5527 3633 2479 1154\n",
      "13: 5293 3682 2506 1176\n",
      "14: 5018 3677 2462 1215\n",
      "15: 4880 3673 2433 1240\n",
      "16: 4597 3579 2285 1294\n",
      "17: 4362 3520 2277 1243\n",
      "18: 4261 3423 2200 1223\n",
      "19: 3973 3391 2127 1264\n",
      "20: 3840 3396 2111 1285\n",
      "21: 3806 3400 2124 1276\n",
      "22: 3583 3263 1999 1264\n",
      "23: 3371 3184 1965 1219\n",
      "24: 3257 3147 1907 1240\n",
      "25: 3109 3068 1858 1210\n",
      "26: 2951 2989 1808 1181\n",
      "27: 2980 3041 1812 1229\n",
      "28: 2790 2870 1702 1168\n",
      "29: 2668 2758 1663 1095\n",
      "30: 2430 2589 1514 1075\n",
      "31: 2300 2548 1459 1089\n",
      "32: 2266 2476 1464 1012\n",
      "33: 2039 2267 1312 955\n",
      "34: 1995 2255 1306 949\n",
      "35: 1780 1964 1129 835\n",
      "36: 1697 1929 1115 814\n",
      "37: 1583 1835 1066 769\n",
      "38: 1598 1834 1033 801\n",
      "39: 1340 1584 912 672\n",
      "40: 1244 1506 857 649\n",
      "41: 1224 1485 809 676\n",
      "42: 1057 1266 718 548\n",
      "43: 982 1159 675 484\n",
      "44: 933 1122 638 484\n",
      "45: 774 903 494 409\n",
      "46: 650 787 430 357\n",
      "47: 581 708 375 333\n",
      "48: 555 673 360 313\n",
      "49: 458 568 304 264\n",
      "50: 381 465 252 213\n",
      "51: 362 415 217 198\n",
      "52: 299 349 190 159\n",
      "53: 259 305 162 143\n",
      "54: 226 277 140 137\n",
      "55: 190 223 121 102\n",
      "56: 147 177 93 84\n",
      "57: 111 144 78 66\n",
      "58: 85 101 53 48\n",
      "59: 87 101 50 51\n",
      "60: 73 95 50 45\n",
      "61: 49 62 30 32\n",
      "62: 42 40 20 20\n",
      "63: 30 37 21 16\n",
      "64: 27 26 12 14\n",
      "65: 19 21 10 11\n",
      "66: 23 22 10 12\n",
      "67: 6 6 2 4\n",
      "68: 9 9 5 4\n",
      "69: 6 7 4 3\n",
      "70: 4 4 3 1\n",
      "71: 1 1 0 1\n",
      "72: 2 2 2 0\n",
      "73: 2 2 0 2\n",
      "74: 4 4 2 2\n",
      "75: 1 1 0 1\n",
      "76: 2 2 1 1\n",
      "77: 1 1 0 1\n",
      "79: 1 2 1 1\n",
      "81: 1 1 0 1\n"
     ]
    }
   ],
   "source": [
    "utterances = UtteranceCollector(SWB_DIR).utterances\n",
    "\n",
    "print('Total:', len(utterances))\n",
    "\n",
    "contain_nls = list(filter(lambda x: contains_nls(x), utterances))\n",
    "print('contain nls:', len(contain_nls))\n",
    "\n",
    "for repetitions in range(1, 10):\n",
    "    contain_repetition = list(filter(lambda x: contains_repetition(x, repetitions), utterances))\n",
    "    print(f'contain {repetitions}-gram repetitions:', len(contain_repetition))\n",
    "    print(random.choice(contain_repetition)['utterance'])\n",
    "\n",
    "print('Lengths:')\n",
    "lengths = {}\n",
    "for utterance in utterances:\n",
    "    utterance_length = len(utterance['utterance'].split(' '))\n",
    "    lengths.setdefault(utterance_length, []).append(utterance)\n",
    "for length, utts in sorted(lengths.items()):\n",
    "    contain_nls = list(filter(lambda x: contains_nls(x), utts))\n",
    "    contain_repetition = list(filter(lambda x: contains_repetition(x), utts))\n",
    "    print(f'{length}:', len(utts), len(contain_nls) + len(contain_repetition), len(contain_nls), len(contain_repetition))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(source_path, target_path_train, target_path_test, number_files=-1, train_split=0.8):\n",
    "    max_length_utterance = -1\n",
    "    \n",
    "    utterances = UtteranceCollector(source_path, number_files).utterances\n",
    "\n",
    "    \n",
    "    delimiter = int(len(utterances) * train_split)\n",
    "\n",
    "    with open(target_path_train, 'w') as target_train:\n",
    "        for utterance in utterances[:delimiter]:\n",
    "            tokenized = nltk.word_tokenize(utterance['utterance'])\n",
    "            max_length_utterance = max(max_length_utterance, len(tokenized))\n",
    "            target_train.write(f\"{utterance['utterance']}\\t{utterance['dialogue_id']}\\t{utterance['dialogue_partner']}\\n\")\n",
    "    with open(target_path_test, 'w') as target_test:\n",
    "        for utterance in utterances[delimiter:]:\n",
    "            tokenized = nltk.word_tokenize(utterance['utterance'])\n",
    "            max_length_utterance = max(max_length_utterance, len(tokenized))\n",
    "            target_test.write(f\"{utterance['utterance']}\\t{utterance['dialogue_id']}\\t{utterance['dialogue_partner']}\\n\")\n",
    "\n",
    "    return max_length_utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_utterance = split_data(SWB_DIR, TRAIN_PATH, TEST_PATH)\n",
    "max_length_utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLSDataset(Dataset):\n",
    "    def __init__(self, path, max_length_utterance=-1, dataset=None) -> None:\n",
    "        super().__init__()\n",
    "        utterances = self._read_file(path)\n",
    "\n",
    "        if dataset is None:\n",
    "            self.max_length_source = max_length_utterance + 2\n",
    "            self.max_length_target = max_length_utterance + 2\n",
    "            vocab = {PADDING_TOKEN, UNKNOWN_TOKEN, START_TOKEN, END_TOKEN, *NLS}\n",
    "\n",
    "            for utterance in utterances:\n",
    "                target = nltk.word_tokenize(utterance['utterance'])\n",
    "                vocab.update(target)\n",
    "\n",
    "                source = [word for word in target if word not in NLS]\n",
    "\n",
    "                self.max_length_source = max(self.max_length_source, len(source))\n",
    "                self.max_length_target = max(self.max_length_target, len(target))\n",
    "\n",
    "            self.vocab = {word: index for index, word in enumerate(list(vocab))}\n",
    "        else:\n",
    "            self.vocab = dataset.vocab\n",
    "            self.max_length_source = dataset.max_length_source\n",
    "            self.max_length_target = dataset.max_length_target\n",
    "\n",
    "        self.samples = []\n",
    "        for utterance in utterances:\n",
    "            tokenized = [START_TOKEN, *nltk.word_tokenize(utterance['utterance']), END_TOKEN]\n",
    "\n",
    "            target = [self.get_encoded_word(word) for word in tokenized]\n",
    "            source = [self.get_encoded_word(word) for word in tokenized if word not in NLS]\n",
    "            \n",
    "            target.extend([self.get_encoded_word(PADDING_TOKEN)] * (self.max_length_target - len(target)))\n",
    "            source.extend([self.get_encoded_word(PADDING_TOKEN)] * (self.max_length_source - len(source)))\n",
    "\n",
    "            try:\n",
    "                self.samples.append({\n",
    "                    'dialogue_id': utterance['dialogue_id'],\n",
    "                    'dialogue_partner': utterance['dialogue_partner'],\n",
    "                    'source': torch.tensor(source),\n",
    "                    'target': torch.tensor(target),\n",
    "                })\n",
    "            except:\n",
    "                print(utterance) \n",
    "\n",
    "    def _read_file(self, path):\n",
    "        utterances = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                utterance, dialogue_id, dialogue_partner = line.rstrip().split('\\t')\n",
    "                utterances.append({\n",
    "                    'utterance': utterance,\n",
    "                    'dialogue_id': dialogue_id,\n",
    "                    'dialogue_partner': dialogue_partner\n",
    "                })\n",
    "        \n",
    "        return utterances\n",
    "\n",
    "    def get_encoded_word(self, word) -> int:\n",
    "        if word in self.vocab:\n",
    "            return self.vocab[word]\n",
    "        else:\n",
    "            return self.vocab[UNKNOWN_TOKEN]\n",
    "\n",
    "    def __getitem__(self, item) -> dict:\n",
    "        return self.samples[item]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NLSDataset(TRAIN_PATH, max_length_utterance)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue_id': '2121',\n",
       " 'dialogue_partner': 'A',\n",
       " 'source': tensor([319, 469, 472, 331, 331, 162, 211, 401, 524, 424, 524, 424, 330, 474,\n",
       "          80, 210,  78, 274, 524, 424, 346, 166, 218, 452, 342, 312,  82, 573,\n",
       "         330, 474,  80,  70, 337, 405,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28]),\n",
       " 'target': tensor([319, 217, 469, 472, 331, 331, 162, 211, 401, 524, 424, 524, 424, 330,\n",
       "         474,  80, 210,  78, 274, 217, 524, 424, 346, 166, 218, 217, 452, 342,\n",
       "         312, 217,  82, 573, 330, 474,  80,  70, 337, 405,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(path_train, path_test, batch_size):\n",
    "    train_dataset = NLSDataset(path_train, max_length_utterance)\n",
    "    test_dataset = NLSDataset(path_test, max_length_utterance, train_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = dataloader(TRAIN_PATH, TEST_PATH, hyperparameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue_id': '2121',\n",
       " 'dialogue_partner': 'A',\n",
       " 'source': tensor([319, 469, 472, 331, 331, 162, 211, 401, 524, 424, 524, 424, 330, 474,\n",
       "          80, 210,  78, 274, 524, 424, 346, 166, 218, 452, 342, 312,  82, 573,\n",
       "         330, 474,  80,  70, 337, 405,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28]),\n",
       " 'target': tensor([319, 217, 469, 472, 331, 331, 162, 211, 401, 524, 424, 524, 424, 330,\n",
       "         474,  80, 210,  78, 274, 217, 524, 424, 346, 166, 218, 217, 452, 342,\n",
       "         312, 217,  82, 573, 330, 474,  80,  70, 337, 405,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,\n",
       "          28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28,  28])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('MLT_non-lexical')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb4abd3088b13a7e0c809898d81beb964403d8b7da59adbc39953b9d8f837bd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
